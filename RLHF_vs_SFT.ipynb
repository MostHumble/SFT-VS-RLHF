{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "RLHF vs SFT"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31090,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install trl"
      ],
      "metadata": {
        "id": "BsRN6yQB0sz5",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Configuration ---\n",
        "COLORS = ['R', 'G', 'B']\n",
        "ALLOWED_TRANSITIONS = {'R': ['G'], 'G': ['B'], 'B': ['R', 'G']}\n",
        "SEQUENCE_LENGTH = 40\n",
        "\n",
        "# New: Dynamic prompt length configuration\n",
        "MIN_PROMPT_LENGTH = 1\n",
        "MAX_PROMPT_LENGTH = 20\n",
        "\n",
        "PRETRAIN_TOTAL_SIZE = 20_000\n",
        "SFT_SIZE = 1000  # Will be derived from DPO data\n",
        "DPO_SIZE = 1000\n",
        "\n",
        "# --- Core Logic (Unchanged) ---\n",
        "\n",
        "def generate_valid_sequence(length, sequence=None):\n",
        "    \"\"\"Generates or continues a valid sequence of colors.\"\"\"\n",
        "    if sequence is None:\n",
        "        # Start with a character that allows for diverse paths\n",
        "        sequence = [random.choice(['R', 'B'])]\n",
        "\n",
        "    # Ensure sequence is a list to allow extension\n",
        "    if isinstance(sequence, str):\n",
        "        sequence = list(sequence)\n",
        "\n",
        "    while len(sequence) < length:\n",
        "        last_color = sequence[-1]\n",
        "        next_color = random.choice(ALLOWED_TRANSITIONS[last_color])\n",
        "        sequence.append(next_color)\n",
        "    return \"\".join(sequence)\n",
        "\n",
        "def calculate_diversity_score(sequence):\n",
        "    \"\"\"Calculates diversity based on the frequency of 'BG' transitions.\"\"\"\n",
        "    bg_transitions = sequence.count('BG')\n",
        "    # Avoid division by zero for single-character sequences\n",
        "    return bg_transitions / (len(sequence) - 1) if len(sequence) > 1 else 0\n",
        "\n",
        "# --- Dataset Generation Functions (Pre-train is unchanged) ---\n",
        "\n",
        "def create_pretrain_dataset():\n",
        "    \"\"\"Generates pre-training data and splits it into train and test sets.\"\"\"\n",
        "    print(f\"Generating {PRETRAIN_TOTAL_SIZE} sequences for pre-training...\")\n",
        "    all_sequences = [generate_valid_sequence(SEQUENCE_LENGTH) for _ in range(PRETRAIN_TOTAL_SIZE)]\n",
        "\n",
        "    train_seqs, test_seqs = train_test_split(all_sequences, test_size=0.1, random_state=42)\n",
        "\n",
        "    with open(\"pretrain_train.jsonl\", 'w') as f:\n",
        "        for seq in train_seqs:\n",
        "            f.write(json.dumps({\"text\": seq}) + \"\\n\")\n",
        "    print(f\"Saved {len(train_seqs)} sequences to pretrain_train.jsonl\")\n",
        "\n",
        "    with open(\"pretrain_test.jsonl\", 'w') as f:\n",
        "        for seq in test_seqs:\n",
        "            f.write(json.dumps({\"text\": seq}) + \"\\n\")\n",
        "    print(f\"Saved {len(test_seqs)} sequences to pretrain_test.jsonl\")\n",
        "\n",
        "\n",
        "# --- REVISED FUNCTION ---\n",
        "def create_dpo_and_sft_datasets():\n",
        "    \"\"\"\n",
        "    Generates DPO and SFT datasets with dynamic-length prompts and a\n",
        "    prompt/completion structure.\n",
        "    \"\"\"\n",
        "    print(f\"Generating {DPO_SIZE} preference pairs for DPO...\")\n",
        "    preference_pairs = []\n",
        "\n",
        "    while len(preference_pairs) < DPO_SIZE:\n",
        "        # 1. Generate a prompt with a dynamic length\n",
        "        prompt_length = random.randint(MIN_PROMPT_LENGTH, MAX_PROMPT_LENGTH)\n",
        "        prompt = generate_valid_sequence(prompt_length)\n",
        "\n",
        "        # 2. Generate two different continuations from that same prompt\n",
        "        seq1_full = generate_valid_sequence(SEQUENCE_LENGTH, sequence=prompt)\n",
        "        seq2_full = generate_valid_sequence(SEQUENCE_LENGTH, sequence=prompt)\n",
        "\n",
        "        # Ensure the generated sequences are actually different\n",
        "        if seq1_full == seq2_full:\n",
        "            continue\n",
        "\n",
        "        # 3. Score the full sequences to determine preference\n",
        "        score1, score2 = calculate_diversity_score(seq1_full), calculate_diversity_score(seq2_full)\n",
        "\n",
        "        # Ensure scores are different to create a clear preference\n",
        "        if score1 == score2:\n",
        "            continue\n",
        "\n",
        "        # 4. Assign chosen and rejected based on the score\n",
        "        chosen_full, rejected_full = (seq1_full, seq2_full) if score1 > score2 else (seq2_full, seq1_full)\n",
        "\n",
        "        # 5. Extract the completions (the part after the prompt)\n",
        "        chosen_completion = chosen_full[len(prompt):]\n",
        "        rejected_completion = rejected_full[len(prompt):]\n",
        "\n",
        "        preference_pairs.append({\n",
        "            \"prompt\": prompt,\n",
        "            \"chosen\": chosen_completion,\n",
        "            \"rejected\": rejected_completion\n",
        "        })\n",
        "\n",
        "    # Save the DPO dataset with the new structure\n",
        "    with open(\"dpo.jsonl\", 'w') as f:\n",
        "        for pair in preference_pairs:\n",
        "            f.write(json.dumps(pair) + \"\\n\")\n",
        "    print(f\"DPO preference dataset saved to dpo.jsonl\")\n",
        "\n",
        "    # --- Create the SFT dataset from the 'chosen' completions ---\n",
        "    print(f\"Generating {SFT_SIZE} examples for SFT...\")\n",
        "    # Sort pairs by the diversity of their 'chosen' sequence\n",
        "    # To do this, we need to reconstruct the full sequence for scoring\n",
        "    preference_pairs.sort(\n",
        "        key=lambda p: calculate_diversity_score(p['prompt'] + p['chosen']),\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    # Take the top SFT_SIZE examples\n",
        "    sft_dataset = preference_pairs[:SFT_SIZE]\n",
        "\n",
        "    # Save the SFT dataset with the prompt/completion structure\n",
        "    with open(\"sft.jsonl\", 'w') as f:\n",
        "        for pair in sft_dataset:\n",
        "            # We only need the prompt and the chosen completion for SFT\n",
        "            sft_entry = {\"text\": pair['prompt'] + pair['chosen']}\n",
        "            f.write(json.dumps(sft_entry) + \"\\n\")\n",
        "    print(f\"SFT dataset with {len(sft_dataset)} sequences saved to sft.jsonl\")\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"--- Starting Updated Dataset Generation ---\")\n",
        "    create_pretrain_dataset()\n",
        "    print(\"-\" * 20)\n",
        "    create_dpo_and_sft_datasets()\n",
        "    print(\"-\" * 20)\n",
        "    print(\"All datasets generated successfully. ✅\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJDaJNc5-_L0",
        "outputId": "b1977ad5-ee51-4868-bcd3-5d9ace535908",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-24T13:00:07.464822Z",
          "iopub.execute_input": "2025-08-24T13:00:07.465513Z",
          "iopub.status.idle": "2025-08-24T13:00:07.874826Z",
          "shell.execute_reply.started": "2025-08-24T13:00:07.465489Z",
          "shell.execute_reply": "2025-08-24T13:00:07.873998Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "--- Starting Updated Dataset Generation ---\nGenerating 20000 sequences for pre-training...\nSaved 18000 sequences to pretrain_train.jsonl\nSaved 2000 sequences to pretrain_test.jsonl\n--------------------\nGenerating 1000 preference pairs for DPO...\nDPO preference dataset saved to dpo.jsonl\nGenerating 1000 examples for SFT...\nSFT dataset with 1000 sequences saved to sft.jsonl\n--------------------\nAll datasets generated successfully. ✅\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login(key=\"api-key\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "20xKWBC8arRZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    GPT2Config,\n",
        "    GPT2LMHeadModel,\n",
        ")\n",
        "from torch.utils.data import Dataset\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "# --- Configuration ---\n",
        "PRETRAIN_FILE = \"pretrain.jsonl\"\n",
        "MODEL_OUTPUT_DIR = \"pretrained-color-model\"\n",
        "\n",
        "# Define the \"grammar\" rules to check against\n",
        "ALLOWED_TRANSITIONS = {\n",
        "    'R': ['G'],\n",
        "    'G': ['B'],\n",
        "    'B': ['R', 'G']\n",
        "}\n",
        "VOCAB = ['R', 'G', 'B']\n",
        "\n",
        "# --- 1. Create a Manual Character-Level Tokenizer ---\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Sequence, Union\n",
        "\n",
        "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
        "\n",
        "class ManualCharTokenizer(PreTrainedTokenizer):\n",
        "    def __init__(self, characters: Sequence[str], model_max_length: int, **kwargs):\n",
        "        \"\"\"Character tokenizer for Hugging Face transformers.\n",
        "\n",
        "        Args:\n",
        "            characters (Sequence[str]): List of desired characters. Any character which\n",
        "                is not included in this list will be replaced by a special token called\n",
        "                [UNK] with id=6. Following are list of all of the special tokens with\n",
        "                their corresponding ids:\n",
        "                    \"[CLS]\": 0\n",
        "                    \"[SEP]\": 1\n",
        "                    \"[BOS]\": 2\n",
        "                    \"[MASK]\": 3\n",
        "                    \"[PAD]\": 4\n",
        "                    \"[RESERVED]\": 5\n",
        "                    \"[UNK]\": 6\n",
        "                an id (starting at 7) will be assigned to each character.\n",
        "\n",
        "            model_max_length (int): Model maximum sequence length.\n",
        "        \"\"\"\n",
        "        self.characters = characters\n",
        "        self.model_max_length = model_max_length\n",
        "        bos_token = AddedToken(\"[BOS]\", lstrip=False, rstrip=False)\n",
        "        eos_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
        "        sep_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
        "        cls_token = AddedToken(\"[CLS]\", lstrip=False, rstrip=False)\n",
        "        pad_token = AddedToken(\"[PAD]\", lstrip=False, rstrip=False)\n",
        "        unk_token = AddedToken(\"[UNK]\", lstrip=False, rstrip=False)\n",
        "\n",
        "        mask_token = AddedToken(\"[MASK]\", lstrip=True, rstrip=False)\n",
        "\n",
        "        self._vocab_str_to_int = {\n",
        "            \"[CLS]\": 0,\n",
        "            \"[SEP]\": 1,\n",
        "            \"[BOS]\": 2,\n",
        "            \"[MASK]\": 3,\n",
        "            \"[PAD]\": 4,\n",
        "            \"[RESERVED]\": 5,\n",
        "            \"[UNK]\": 6,\n",
        "            **{ch: i + 7 for i, ch in enumerate(characters)},\n",
        "        }\n",
        "        self._vocab_int_to_str = {v: k for k, v in self._vocab_str_to_int.items()}\n",
        "\n",
        "        super().__init__(\n",
        "            bos_token=bos_token,\n",
        "            eos_token=eos_token,\n",
        "            sep_token=sep_token,\n",
        "            cls_token=cls_token,\n",
        "            pad_token=pad_token,\n",
        "            mask_token=mask_token,\n",
        "            unk_token=unk_token,\n",
        "            add_prefix_space=False,\n",
        "            model_max_length=model_max_length,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        return len(self._vocab_str_to_int)\n",
        "\n",
        "    def get_vocab(self):\n",
        "        return self._vocab_str_to_int\n",
        "\n",
        "    def _tokenize(self, text: str) -> List[str]:\n",
        "        return list(text)\n",
        "\n",
        "    def _convert_token_to_id(self, token: str) -> int:\n",
        "        return self._vocab_str_to_int.get(token, self._vocab_str_to_int[\"[UNK]\"])\n",
        "\n",
        "    def _convert_id_to_token(self, index: int) -> str:\n",
        "        return self._vocab_int_to_str[index]\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        return \"\".join(tokens)\n",
        "\n",
        "    def build_inputs_with_special_tokens(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        result = cls + token_ids_0 + sep\n",
        "        if token_ids_1 is not None:\n",
        "            result += token_ids_1 + sep\n",
        "        return result\n",
        "\n",
        "    def get_special_tokens_mask(\n",
        "        self,\n",
        "        token_ids_0: List[int],\n",
        "        token_ids_1: Optional[List[int]] = None,\n",
        "        already_has_special_tokens: bool = False,\n",
        "    ) -> List[int]:\n",
        "        if already_has_special_tokens:\n",
        "            return super().get_special_tokens_mask(\n",
        "                token_ids_0=token_ids_0,\n",
        "                token_ids_1=token_ids_1,\n",
        "                already_has_special_tokens=True,\n",
        "            )\n",
        "\n",
        "        result = [1] + ([0] * len(token_ids_0)) + [1]\n",
        "        if token_ids_1 is not None:\n",
        "            result += ([0] * len(token_ids_1)) + [1]\n",
        "        return result\n",
        "\n",
        "    def create_token_type_ids_from_sequences(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "\n",
        "        result = len(cls + token_ids_0 + sep) * [0]\n",
        "        if token_ids_1 is not None:\n",
        "            result += len(token_ids_1 + sep) * [1]\n",
        "        return result\n",
        "\n",
        "    def get_config(self) -> Dict:\n",
        "        return {\n",
        "            \"char_ords\": [ord(ch) for ch in self.characters],\n",
        "            \"model_max_length\": self.model_max_length,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config: Dict) -> \"CharacterTokenizer\":\n",
        "        cfg = {}\n",
        "        cfg[\"characters\"] = [chr(i) for i in config[\"char_ords\"]]\n",
        "        cfg[\"model_max_length\"] = config[\"model_max_length\"]\n",
        "        return cls(**cfg)\n",
        "\n",
        "    def save_pretrained(self, save_directory: Union[str, os.PathLike], **kwargs):\n",
        "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
        "        cfg = self.get_config()\n",
        "        with open(cfg_file, \"w\") as f:\n",
        "            json.dump(cfg, f, indent=4)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, save_directory: Union[str, os.PathLike], **kwargs):\n",
        "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
        "        with open(cfg_file) as f:\n",
        "            cfg = json.load(f)\n",
        "        return cls.from_config(cfg)\n",
        "\n",
        "# Instantiate our new tokenizer\n",
        "chars = VOCAB # This is character vocab\n",
        "model_max_length = 2048\n",
        "tokenizer = ManualCharTokenizer(chars, model_max_length)\n",
        "\n",
        "print(\"Manual character-level tokenizer created.\")\n",
        "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
        "\n",
        "\n",
        "# --- 2. Load and Prepare the Dataset ---\n",
        "\n",
        "# Load the dataset from the JSONL file\n",
        "# Assuming you have split your data into train and test files\n",
        "train_dataset = load_dataset('json', data_files=\"pretrain_train.jsonl\", split='train')\n",
        "test_dataset = load_dataset('json', data_files=\"pretrain_test.jsonl\", split='train')\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    # Our tokenizer's __call__ method handles batching and returns unpadded input_ids\n",
        "    return tokenizer(examples['text'], truncation=True, max_length=512)\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
        "\n",
        "# Create a data collator for causal language modeling\n",
        "# This collator will now use our tokenizer's .pad() method correctly\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "\n",
        "# --- 3. Define the Model ---\n",
        "\n",
        "# We'll use a small GPT-2 model configured for our tiny vocabulary\n",
        "model_config = GPT2Config(\n",
        "    model_type=\"gpt2\",\n",
        "    vocab_size=len(tokenizer),\n",
        "    n_positions=512,\n",
        "    n_embd=256, # Smaller embedding size\n",
        "    n_layer=6,  # Fewer layers\n",
        "    n_head=4,   # Fewer attention heads\n",
        "    tie_word_embeddings=False\n",
        ")\n",
        "\n",
        "model = GPT2LMHeadModel(config=model_config)\n",
        "print(f\"Model initialized with {model.num_parameters():,} parameters.\")\n",
        "\n",
        "\n",
        "# --- 4. Define the Metric Calculation ---\n",
        "\n",
        "def calculate_local_rule_adherence(model, tokenizer, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    \"\"\"\n",
        "    Generates a sequence and calculates the percentage of valid transitions.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    model.eval() # Set model to evaluation mode\n",
        "\n",
        "    # Generate a long sequence to test adherence\n",
        "    start_char = random.choice(['R', 'B'])\n",
        "    start_token = torch.tensor([tokenizer.encode(start_char)], device=device)\n",
        "\n",
        "    # generate() is a powerful method for autoregressive generation\n",
        "    generated_ids = model.generate(\n",
        "        start_token,\n",
        "        max_length=40,\n",
        "        do_sample=True,\n",
        "        top_k=5, # Limit sampling to top-k tokens to avoid bad choices\n",
        "        pad_token_id=tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(generated_ids[0].tolist(), skip_special_tokens=True)\n",
        "\n",
        "    # Calculate adherence\n",
        "    valid_transitions = 0\n",
        "    total_transitions = len(generated_text) - 1\n",
        "\n",
        "    if total_transitions <= 0:\n",
        "        return 0.0\n",
        "\n",
        "    for i in range(total_transitions):\n",
        "        current_char = generated_text[i]\n",
        "        next_char = generated_text[i+1]\n",
        "\n",
        "        if current_char in ALLOWED_TRANSITIONS and next_char in ALLOWED_TRANSITIONS[current_char]:\n",
        "            valid_transitions += 1\n",
        "\n",
        "    adherence = (valid_transitions / total_transitions) * 100\n",
        "    return adherence\n",
        "\n",
        "# We need a custom compute_metrics function for the Trainer\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    This function will be called by the Trainer at each evaluation.\n",
        "    \"\"\"\n",
        "    adherence_score = calculate_local_rule_adherence(model, tokenizer)\n",
        "    return {\"local_rule_adherence\": adherence_score}\n",
        "\n",
        "\n",
        "# --- 5. Train the Model ---\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=MODEL_OUTPUT_DIR,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=256,\n",
        "    save_steps=18,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=18,\n",
        "    eval_strategy=\"steps\", # Renamed from eval_strategy\n",
        "    eval_steps=18,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"local_rule_adherence\",\n",
        "    greater_is_better=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"--- Starting Pre-training ---\")\n",
        "trainer.train()\n",
        "\n",
        "# --- 6. Save Final Model and Tokenizer ---\n",
        "print(\"--- Pre-training Complete ---\")\n",
        "trainer.save_model(MODEL_OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(MODEL_OUTPUT_DIR)\n",
        "print(f\"Final model and tokenizer saved to {MODEL_OUTPUT_DIR}\")\n",
        "\n",
        "# --- 7. Final Test of the Trained Model ---\n",
        "print(\"\\n--- Testing Final Model ---\")\n",
        "final_adherence = calculate_local_rule_adherence(model, tokenizer)\n",
        "print(f\"Final Local Rule Adherence: {final_adherence:.2f}%\")\n",
        "print(\"Generating an example sequence:\")\n",
        "\n",
        "# Generate one last sample\n",
        "start_char = random.choice(['R', 'B'])\n",
        "start_token = torch.tensor([tokenizer.encode(start_char)], device=model.device)\n",
        "generated_ids = model.generate(start_token, max_length=40, do_sample=False, pad_token_id=tokenizer.pad_token_id)\n",
        "print(tokenizer.decode(generated_ids[0].tolist(), skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462,
          "referenced_widgets": [
            "8374518270174cce9081ad8a2e6097f9",
            "2daece5f917a46e082ad94a8fbfa4bd4",
            "81848bea31fe4a408af2b6c61a20f44b",
            "9a63041eb2764b1c8329b432e455c28a",
            "3644b43c50054c6a85b670782e577edc",
            "b5edc812a87444348aed72d5d69db683",
            "b6f9c0e85b0e447ab63be198bfec2224",
            "baa07b8e45fc44b6ae7e8378021bff24",
            "d91d51ca2fd245c2bc2412a26c375898",
            "93b1f34415c94df2945d7250fa95ae56",
            "08dd004bdf914ebf84370d3570c93c53",
            "ce231d6084b5464e937bda158ee22995",
            "1494482360a3447e9b51aad0baad764a",
            "2b54e534e8af477485fbba170fbb2d9a",
            "ee12101787c04710a3eb00c14b40a043",
            "c972e0e3c80a41b58f6f2e46a375dc31",
            "623e0a55ec4f4d3da935de793d9e8259",
            "88f2e35389d14b6f955dbf2aee1a7cc5",
            "ad754c33b9e249e785945d4eb6a3d055",
            "1217a0df0884425ca2ab2d013ddf3f8e",
            "b37b316a8fd741a39620fd2d10443537",
            "e5e6e69b557241b291f9d20aa6486bd6"
          ]
        },
        "id": "9rt1ZgPG_TGd",
        "outputId": "b3883154-235d-4f56-99bd-487bf83e32a5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-24T13:16:40.89475Z",
          "iopub.execute_input": "2025-08-24T13:16:40.895407Z",
          "iopub.status.idle": "2025-08-24T13:17:03.816502Z",
          "shell.execute_reply.started": "2025-08-24T13:16:40.895386Z",
          "shell.execute_reply": "2025-08-24T13:17:03.815846Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Manual character-level tokenizer created.\nVocabulary size: 10\nModel initialized with 4,875,264 parameters.\n--- Starting Pre-training ---\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [71/71 00:20, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Local Rule Adherence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>18</td>\n      <td>0.652700</td>\n      <td>0.372674</td>\n      <td>100.000000</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.343800</td>\n      <td>0.328805</td>\n      <td>97.222222</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>0.323400</td>\n      <td>0.321196</td>\n      <td>94.285714</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "--- Pre-training Complete ---\nFinal model and tokenizer saved to pretrained-color-model\n\n--- Testing Final Model ---\nFinal Local Rule Adherence: 72.09%\nGenerating an example sequence:\nRBGBGBGBGBGBGBGBGBGBGBGBGBGBGBGBGBGBGB\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "\n",
        "# --- Configuration ---\n",
        "PRETRAINED_MODEL_DIR = \"pretrained-color-model\"\n",
        "SFT_DATA_FILE = \"sft.jsonl\"\n",
        "SFT_OUTPUT_DIR = \"sft-color-model\"\n",
        "\n",
        "# Rules needed for metrics\n",
        "ALLOWED_TRANSITIONS = {'R': ['G'], 'G': ['B'], 'B': ['R', 'G']}\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(PRETRAINED_MODEL_DIR)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "\n",
        "# --- 2. Load and Prepare SFT Dataset ---\n",
        "sft_dataset = load_dataset('json', data_files=SFT_DATA_FILE, split='train')\n",
        "\n",
        "tokenized_sft_dataset = sft_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "def generate_and_evaluate(model, tokenizer):\n",
        "    \"\"\"Generates a sequence and calculates key metrics.\"\"\"\n",
        "    model.to(device)\n",
        "    model.eval() # Set model to evaluation mode\n",
        "\n",
        "    # Generate a long sequence to test adherence\n",
        "    start_char = random.choice(['R', 'B'])\n",
        "    start_token = torch.tensor([tokenizer.encode(start_char)], device=device)\n",
        "\n",
        "    # generate() is a powerful method for autoregressive generation\n",
        "    generated_ids = model.generate(\n",
        "        start_token,\n",
        "        max_length=40,\n",
        "        do_sample=True,\n",
        "        top_k=5, # Limit sampling to top-k tokens to avoid bad choices\n",
        "        pad_token_id=tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(generated_ids[0].tolist(), skip_special_tokens=True)\n",
        "\n",
        "    # Calculate adherence\n",
        "    valid_transitions = 0\n",
        "    total_transitions = len(generated_text) - 1\n",
        "\n",
        "    if total_transitions <= 0:\n",
        "        return 0.0\n",
        "\n",
        "    for i in range(total_transitions):\n",
        "        current_char = generated_text[i]\n",
        "        next_char = generated_text[i+1]\n",
        "\n",
        "        if current_char in ALLOWED_TRANSITIONS and next_char in ALLOWED_TRANSITIONS[current_char]:\n",
        "            valid_transitions += 1\n",
        "\n",
        "    adherence = (valid_transitions / total_transitions) * 100\n",
        "\n",
        "    # Metric 2: Diversity Score\n",
        "    diversity = calculate_diversity_score(generated_text)\n",
        "\n",
        "    return {\"local_rule_adherence\": adherence, \"global_diversity_score\": diversity}\n",
        "\n",
        "def compute_metrics_next(eval_pred):\n",
        "    return generate_and_evaluate(model, tokenizer)\n",
        "\n",
        "# --- 4. Train the Model ---\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=SFT_OUTPUT_DIR,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=10, # Fewer epochs for fine-tuning\n",
        "    per_device_train_batch_size=512,\n",
        "    save_steps=6,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=6,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=6,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"global_diversity_score\",\n",
        "    greater_is_better=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_sft_dataset,\n",
        "    eval_dataset=tokenized_sft_dataset, # Evaluate on the same small dataset\n",
        "    compute_metrics=compute_metrics_next,\n",
        ")\n",
        "\n",
        "print(\"--- Starting Supervised Fine-Tuning (SFT) ---\")\n",
        "trainer.train()\n",
        "\n",
        "# --- 5. Save Final Model ---\n",
        "trainer.save_model(SFT_OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(SFT_OUTPUT_DIR)\n",
        "print(f\"SFT model saved to {SFT_OUTPUT_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "c5843333a8d64f3da6a82647a863876b",
            "43bb10ae76a14b768bdd27040386e1a9",
            "3a983a2435d64bd08afe1eb2800fea33",
            "51dc9ee1ec5d4759b6029d2161b83d24",
            "a2d07d3611794e3995271b74f3cb7374",
            "0c0653068cf24c5db046f6a485610623",
            "de23b8d50ddd4984a227221b43d803ad",
            "e3183d03db484df69e885037c67556ef",
            "a9cf1dfd4d9c406395c7702b4d825b6c",
            "518d90a54dfd4c328a3a84a635893d91",
            "588dc20015404e4fb9c0afc721e6bf67",
            "466e57bb05cc4c8694abc774838c7c6a"
          ]
        },
        "id": "keHdL_fjXC3H",
        "outputId": "af7402d1-e0cf-4514-fd2f-30334cfe3791",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-24T13:04:22.156553Z",
          "iopub.execute_input": "2025-08-24T13:04:22.157162Z",
          "iopub.status.idle": "2025-08-24T13:04:35.003997Z",
          "shell.execute_reply.started": "2025-08-24T13:04:22.157138Z",
          "shell.execute_reply": "2025-08-24T13:04:35.003299Z"
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "466e57bb05cc4c8694abc774838c7c6a"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "--- Starting Supervised Fine-Tuning (SFT) ---\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [20/20 00:11, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Local Rule Adherence</th>\n      <th>Global Diversity Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>6</td>\n      <td>0.443400</td>\n      <td>0.398082</td>\n      <td>97.297297</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.369200</td>\n      <td>0.352447</td>\n      <td>97.297297</td>\n      <td>0.486486</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.345300</td>\n      <td>0.337946</td>\n      <td>97.297297</td>\n      <td>0.486486</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "SFT model saved to sft-color-model\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "# Using a public model for reproducibility.\n",
        "# Ensure your JSONL file has \"prompt\", \"chosen\", and \"rejected\" fields.\n",
        "DPO_DATA_FILE = \"dpo.jsonl\"\n",
        "# Directory where the final DPO model will be saved.\n",
        "DPO_MODEL_OUTPUT_DIR = \"dpo-model-output\"\n",
        "\n",
        "# --- 1. Load Model and Tokenizer ---\n",
        "print(\"Loading pre-trained model and tokenizer...\")\n",
        "\n",
        "# For DPOTrainer, we need the base model which will be trained.\n",
        "model_dpo = AutoModelForCausalLM.from_pretrained(PRETRAINED_MODEL_DIR)\n",
        "model_dpo.to(device)\n",
        "\n",
        "# --- 2. Load the Preference Dataset ---\n",
        "print(\"Loading preference dataset for DPO...\")\n",
        "dpo_dataset = load_dataset('json', data_files=DPO_DATA_FILE, split='train')\n",
        "\n",
        "# --- 3. Set up Training Arguments using DPOConfig ---\n",
        "# DPOConfig is a subclass of TrainingArguments with DPO-specific settings.\n",
        "training_args = DPOConfig(\n",
        "    output_dir=DPO_MODEL_OUTPUT_DIR,\n",
        "    per_device_train_batch_size=2, # DPO can be more memory intensive\n",
        "    num_train_epochs=1,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=5e-6, # Lower learning rate is often better for DPO\n",
        "    report_to=\"tensorboard\",\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"no\",\n",
        "    save_strategy=\"epoch\",\n",
        "    # The beta parameter is a key hyperparameter in DPO.\n",
        "    # It controls how much the model stays close to the reference model.\n",
        "    # A common value is 0.1.\n",
        "    beta=0.1,\n",
        ")\n",
        "\n",
        "# --- 4. Initialize and Run the DPOTrainer ---\n",
        "print(\"--- Starting Direct Preference Optimization (DPO) Training ---\")\n",
        "\n",
        "# The DPOTrainer handles the DPO loss calculation automatically.\n",
        "# It requires the model, an optional ref_model, the tokenizer,\n",
        "# training args, and the dataset.\n",
        "dpo_tokenizer = copy.deepcopy(tokenizer)\n",
        "\n",
        "trainer = DPOTrainer(\n",
        "    model=model_dpo,\n",
        "    ref_model=None, # If None, the trainer will create a reference model internally\n",
        "    args=training_args,\n",
        "    train_dataset=dpo_dataset,\n",
        "    processing_class=dpo_tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# --- 5. Save the Final Model ---\n",
        "print(\"--- DPO Training Complete ---\")\n",
        "trainer.save_model(DPO_MODEL_OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(DPO_MODEL_OUTPUT_DIR)\n",
        "print(f\"DPO model and tokenizer saved to {DPO_MODEL_OUTPUT_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388,
          "referenced_widgets": [
            "18e4cbd49545498aa6dfe8a187758bce",
            "417ddd9546324a4483cc435bab8b9a21",
            "7e06df16626545e0b4c0939a3b147a1a",
            "1f25fddc9a5140d8a9faa78d5f9369db",
            "1ed36af9814340938573db44113a7261",
            "cb6c1767f85c400bacbed9f42d6a9e4c",
            "7b0d5c0e7ee4489896f168ca87ee4d89",
            "2cd7dd88c3944f3cad1c082c17e8eb04",
            "8666bbcb73c44abd946cc6998b001cb6",
            "d75d31d8d1104da1b62c9b83ca34c413",
            "f9115a97289d455fa8cce90191765b01"
          ]
        },
        "id": "F9-LShOhnt5Q",
        "outputId": "39752bb0-a506-422d-83d8-c44e58390fd4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-24T13:17:03.817686Z",
          "iopub.execute_input": "2025-08-24T13:17:03.818294Z",
          "iopub.status.idle": "2025-08-24T13:17:23.370888Z",
          "shell.execute_reply.started": "2025-08-24T13:17:03.818274Z",
          "shell.execute_reply": "2025-08-24T13:17:23.370155Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Loading pre-trained model and tokenizer...\nLoading preference dataset for DPO...\n--- Starting Direct Preference Optimization (DPO) Training ---\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 00:18, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>0.381400</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.195000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.163700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.144900</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.128300</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "--- DPO Training Complete ---\nDPO model and tokenizer saved to dpo-model-output\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "from transformers import AutoModelForCausalLM\n",
        "from scipy.stats import entropy\n",
        "\n",
        "# --- Configuration ---\n",
        "SFT_MODEL_DIR = \"sft-color-model\"           # Directory where the SFT model was saved\n",
        "RLHF_MODEL_DIR = \"dpo-model-output\"         # Directory where the RLHF model was saved\n",
        "VOCAB = ['R', 'G', 'B']                     # The vocabulary for our tokenizer\n",
        "NUM_SAMPLES = 200                           # Number of sequences to generate for evaluation\n",
        "\n",
        "# --- Helper Functions ---\n",
        "ALLOWED_TRANSITIONS = {'R': ['G'], 'G': ['B'], 'B': ['R', 'G']}\n",
        "\n",
        "def calculate_diversity_score(sequence: str) -> float:\n",
        "    \"\"\"Calculates the frequency of the complexity-inducing 'B->G' transition.\"\"\"\n",
        "    bg_transitions = sequence.count('BG')\n",
        "    return bg_transitions / (len(sequence) - 1) if len(sequence) > 1 else 0.0\n",
        "\n",
        "def calculate_rule_adherence(sequence: str) -> float:\n",
        "    \"\"\"Calculates the percentage of valid transitions in a sequence.\"\"\"\n",
        "    valid_transitions = 0\n",
        "    total_transitions = len(sequence) - 1\n",
        "    if total_transitions <= 0:\n",
        "        return 100.0\n",
        "    for i in range(total_transitions):\n",
        "        current_char = sequence[i]\n",
        "        next_char = sequence[i+1]\n",
        "        if current_char in ALLOWED_TRANSITIONS and next_char in ALLOWED_TRANSITIONS[current_char]:\n",
        "            valid_transitions += 1\n",
        "    return (valid_transitions / total_transitions) * 100.0\n",
        "\n",
        "def generate_samples(model, tokenizer, num_samples: int) -> list[str]:\n",
        "    \"\"\"Generates a list of sequences from a given model.\"\"\"\n",
        "    model.eval()\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model.to(device)\n",
        "\n",
        "    sequences = []\n",
        "    print(f\"Generating {num_samples} samples...\")\n",
        "    for _ in range(num_samples):\n",
        "        # Start generation with a random valid character\n",
        "        start_char = random.choice(['R', 'B'])\n",
        "        input_ids = torch.tensor([tokenizer.encode(start_char)], device=device)\n",
        "\n",
        "        # Generate sequence\n",
        "        generated_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_length=40,\n",
        "            do_sample=True,\n",
        "            top_k=5,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "        # Decode and add to the list\n",
        "        seq = tokenizer.decode(generated_ids[0].tolist(), skip_special_tokens=True)\n",
        "        sequences.append(seq)\n",
        "    return sequences\n",
        "\n",
        "def analyze_results(model_name: str, sequences: list[str]):\n",
        "    \"\"\"Calculates and prints all key metrics for a list of generated sequences.\"\"\"\n",
        "    # Metric 1: Global \"Preference\" Score (The primary task goal)\n",
        "    diversity_scores = [calculate_diversity_score(s) for s in sequences]\n",
        "    avg_diversity = np.mean(diversity_scores)\n",
        "    max_diversity = np.max(diversity_scores)\n",
        "\n",
        "    # Metric 2: Local Rule Adherence (Sanity check)\n",
        "    adherence_scores = [calculate_rule_adherence(s) for s in sequences]\n",
        "    avg_adherence = np.mean(adherence_scores)\n",
        "\n",
        "    # Metric 3: Output Entropy (Creativity / Variety)\n",
        "    unique_sequences, counts = np.unique(sequences, return_counts=True)\n",
        "    probs = counts / len(sequences)\n",
        "    output_entropy = entropy(probs, base=2)\n",
        "\n",
        "    print(f\"--- Results for {model_name} ---\")\n",
        "    print(f\"Average Global Preference Scored: {avg_diversity:.4f}\")\n",
        "    print(f\"Maximum Diversity Score Found:  {max_diversity:.4f}\")\n",
        "    print(f\"Average Local Rule Adherence:   {avg_adherence:.2f}%\")\n",
        "    print(f\"Output Entropy:    {output_entropy:.4f} bits\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# --- Main Comparison Script ---\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Load the BASE model\n",
        "    print(f\"Loading BASE model from {MODEL_OUTPUT_DIR}...\")\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(MODEL_OUTPUT_DIR)\n",
        "    base_model.to(device)\n",
        "\n",
        "    # Load the SFT model\n",
        "    print(f\"Loading SFT model from {SFT_MODEL_DIR}...\")\n",
        "    sft_model = AutoModelForCausalLM.from_pretrained(SFT_MODEL_DIR)\n",
        "    sft_model.to(device)\n",
        "\n",
        "    # Load the DPO model\n",
        "    print(f\"Loading DPO model from {DPO_MODEL_OUTPUT_DIR}...\")\n",
        "    dpo_model = AutoModelForCausalLM.from_pretrained(DPO_MODEL_OUTPUT_DIR)\n",
        "    dpo_model.to(device)\n",
        "\n",
        "    # Generate sequences from both models\n",
        "    sft_sequences = generate_samples(sft_model, tokenizer, NUM_SAMPLES)\n",
        "    dpo_sequences = generate_samples(dpo_model, tokenizer, NUM_SAMPLES)\n",
        "    base_sequences = generate_samples(base_model, tokenizer, NUM_SAMPLES)\n",
        "\n",
        "    # Analyze and print the final comparison\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"           FINAL MODEL COMPARISON\")\n",
        "    print(\"=\"*40)\n",
        "    analyze_results(\"SFT Model (The Imitator)\", sft_sequences)\n",
        "    analyze_results(\"DPO Model (The Explorer)\", dpo_sequences)\n",
        "    analyze_results(\"BASE Model (The Base)\", base_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBseOR08rS8Z",
        "outputId": "7ad34ed4-ef26-44c0-a1fb-1d4f78889c60",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-24T13:21:19.461297Z",
          "iopub.execute_input": "2025-08-24T13:21:19.461614Z",
          "iopub.status.idle": "2025-08-24T13:23:21.727079Z",
          "shell.execute_reply.started": "2025-08-24T13:21:19.46159Z",
          "shell.execute_reply": "2025-08-24T13:23:21.726305Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Loading BASE model from pretrained-color-model...\nLoading SFT model from sft-color-model...\nLoading DPO model from dpo-model-output...\nGenerating 200 samples...\nGenerating 200 samples...\nGenerating 200 samples...\n\n========================================\n           FINAL MODEL COMPARISON\n========================================\n--- Results for SFT Model (The Imitator) ---\nAverage Global Preference Scored: 0.2966\nMaximum Diversity Score Found:  0.4595\nAverage Local Rule Adherence:   92.62%\nOutput Entropy:    7.6439 bits\n----------------------------------------\n--- Results for DPO Model (The Explorer) ---\nAverage Global Preference Scored: 0.4551\nMaximum Diversity Score Found:  0.5000\nAverage Local Rule Adherence:   88.44%\nOutput Entropy:    6.6649 bits\n----------------------------------------\n--- Results for BASE Model (The Base) ---\nAverage Global Preference Scored: 0.2434\nMaximum Diversity Score Found:  0.4118\nAverage Local Rule Adherence:   86.66%\nOutput Entropy:    7.6439 bits\n----------------------------------------\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Configuration ---\n",
        "# Dictionary of model names and their corresponding directory paths\n",
        "MODEL_DIRS = {\n",
        "    \"BASE\": \"pretrained-color-model\",\n",
        "    \"SFT\": \"sft-color-model\",\n",
        "    \"DPO\": \"dpo-model-output\"\n",
        "}\n",
        "\n",
        "# We will generate 20 sequences of 20 tokens for each model\n",
        "NUM_SEQUENCES_PER_MODEL = {\n",
        "    \"BASE\": 20,\n",
        "    \"SFT\": 20,\n",
        "    \"DPO\": 20\n",
        "}\n",
        "SEQUENCE_LENGTH = 20\n",
        "\n",
        "OUTPUT_FILENAME = \"model_comparison_subplots.png\"\n",
        "\n",
        "# Define the mapping from characters to RGB colors\n",
        "COLOR_MAP = {\n",
        "    'R': (255, 0, 0),     # Red\n",
        "    'G': (0, 255, 0),     # Green\n",
        "    'B': (0, 0, 255),     # Blue\n",
        "    '[UNK]': (0, 0, 0),   # Black for unknown/special tokens\n",
        "}\n",
        "\n",
        "def generate_sequences(model, tokenizer, device, num_to_generate, length):\n",
        "    \"\"\"Generates a specified number of sequences from a given model.\"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    sequences = []\n",
        "    print(f\"Generating {num_to_generate} sequences of length {length}...\")\n",
        "    for _ in range(num_to_generate):\n",
        "        start_char = random.choice(['R', 'B'])\n",
        "        input_ids = torch.tensor([tokenizer.encode(start_char)], device=device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                input_ids,\n",
        "                max_length=length,\n",
        "                do_sample=True,\n",
        "                top_k=5,\n",
        "                pad_token_id=tokenizer.pad_token_id\n",
        "            )\n",
        "        sequence = tokenizer.decode(generated_ids[0].tolist(), skip_special_tokens=True)\n",
        "        # Ensure the sequence is exactly the desired length for the image grid\n",
        "        # Note: .ljust is a string method and pads with the character 'R' here.\n",
        "        sequences.append(sequence.ljust(length, 'R')[:length]) # Pad if shorter, truncate if longer\n",
        "\n",
        "    return sequences\n",
        "\n",
        "def create_subplot_image(model_sequences, filename):\n",
        "    \"\"\"\n",
        "    Creates a 1x3 subplot image from sequences generated by different models.\n",
        "    Each subplot is a 20x20 grid representing the model's output.\n",
        "    \"\"\"\n",
        "    num_models = len(model_sequences)\n",
        "\n",
        "    # Create a figure and a set of subplots (1 row, num_models columns)\n",
        "    # The figsize is adjusted for better aesthetics\n",
        "    fig, axes = plt.subplots(1, num_models, figsize=(num_models * 4, 5))\n",
        "\n",
        "    # Ensure 'axes' is an array even if there's only one model\n",
        "    if num_models == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    print(\"Creating subplot image...\")\n",
        "\n",
        "    # Iterate through the models' sequences and the corresponding axes\n",
        "    for ax, (model_name, sequences) in zip(axes, model_sequences.items()):\n",
        "        # Convert character sequences to a list of RGB pixel rows\n",
        "        pixel_rows = []\n",
        "        for seq in sequences:\n",
        "            # Ensure sequence is exactly SEQUENCE_LENGTH and map chars to colors\n",
        "            row = [COLOR_MAP.get(char, COLOR_MAP['[UNK]']) for char in seq[:SEQUENCE_LENGTH]]\n",
        "            pixel_rows.append(row)\n",
        "\n",
        "        # Create a NumPy array from the pixel data. Shape will be (20, 20, 3)\n",
        "        image_array = np.array(pixel_rows, dtype=np.uint8)\n",
        "\n",
        "        # Display the image on the current subplot\n",
        "        ax.imshow(image_array)\n",
        "\n",
        "        # Set the title for the subplot\n",
        "        ax.set_title(model_name, fontsize=30, pad=10)\n",
        "\n",
        "        # Turn off the axis ticks and labels for a cleaner look\n",
        "        ax.axis('off')\n",
        "\n",
        "    # Adjust layout to prevent titles/plots from overlapping and add a main title\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95]) # Adjust rect to make space for suptitle\n",
        "\n",
        "    # Save the entire figure to a file\n",
        "    fig.savefig(filename, dpi=300)\n",
        "    plt.show()\n",
        "    print(f\"\\nSubplot image saved as '{filename}'\")\n",
        "    plt.close(fig) # Close the figure to free up memory\n",
        "\n",
        "\"\"\"Main function to load models, generate sequences, and create the image.\"\"\"\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- 1. Load Tokenizer (it's the same for all models) ---\n",
        "# This assumes you have your custom ManualCharTokenizer class available.\n",
        "# If not, you might need to adjust this part.\n",
        "tokenizer_path = MODEL_DIRS[\"DPO\"]\n",
        "print(f\"Loading tokenizer from {tokenizer_path}...\")\n",
        "# NOTE: The user's original script had a custom tokenizer.\n",
        "# This line is kept as is. Make sure ManualCharTokenizer is defined/imported.\n",
        "# from your_tokenizer_file import ManualCharTokenizer\n",
        "tokenizer = ManualCharTokenizer.from_pretrained(tokenizer_path)\n",
        "\n",
        "# --- 2. Load Models ---\n",
        "models = {}\n",
        "for name, path in MODEL_DIRS.items():\n",
        "    if not os.path.isdir(path):\n",
        "        print(f\"Error: Model directory '{path}' for model '{name}' not found.\")\n",
        "        pass\n",
        "    print(f\"Loading {name} model from {path}...\")\n",
        "    models[name] = AutoModelForCausalLM.from_pretrained(path)\n",
        "\n",
        "# --- 3. Generate Sequences from Each Model ---\n",
        "# We'll store sequences in a dictionary to associate them with their model\n",
        "all_model_sequences = {}\n",
        "for name, model in models.items():\n",
        "    num_seqs = NUM_SEQUENCES_PER_MODEL[name]\n",
        "    print(f\"\\n--- Generating for {name} Model ---\")\n",
        "    sequences = generate_sequences(model, tokenizer, device, num_seqs, SEQUENCE_LENGTH)\n",
        "    all_model_sequences[name] = sequences\n",
        "\n",
        "# --- 4. Create and save the subplot image ---\n",
        "create_subplot_image(all_model_sequences, OUTPUT_FILENAME)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-24T13:28:30.266112Z",
          "iopub.execute_input": "2025-08-24T13:28:30.266421Z",
          "iopub.status.idle": "2025-08-24T13:28:36.954114Z",
          "shell.execute_reply.started": "2025-08-24T13:28:30.266402Z",
          "shell.execute_reply": "2025-08-24T13:28:36.953348Z"
        },
        "id": "lXH-ZOsNarRf",
        "outputId": "bcfc80f0-369a-4a60-9a48-0494d1cdaca2"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Using device: cuda\nLoading tokenizer from dpo-model-output...\nLoading BASE model from pretrained-color-model...\nLoading SFT model from sft-color-model...\nLoading DPO model from dpo-model-output...\n\n--- Generating for BASE Model ---\nGenerating 20 sequences of length 20...\n\n--- Generating for SFT Model ---\nGenerating 20 sequences of length 20...\n\n--- Generating for DPO Model ---\nGenerating 20 sequences of length 20...\nCreating subplot image...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 1200x500 with 3 Axes>",
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\nSubplot image saved as 'model_comparison_subplots.png'\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}
